{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b074b4c5",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Music is a form of art that is ubiquitous and has a rich history. Different composers have created music with their unique styles and compositions. However, identifying the composer of a particular piece of music can be a challenging task, especially for novice musicians or listeners. The proposed project aims to use deep learning techniques to identify the composer of a given piece of music accurately.\n",
    "\n",
    "# Objective\n",
    "\n",
    "The primary objective of this project is to develop a deep learning model that can predict the composer of a given musical score accurately. The project aims to accomplish this objective by using two deep learning techniques: Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN).\n",
    "\n",
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "346e3250",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Imports\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from miditok.data_augmentation import augment_dataset\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from miditok.utils import split_files_for_training\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from random import sample, shuffle, seed as random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "808a4605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Set seeds and device\"\"\"\n",
    "random_seed(73)\n",
    "np.random.seed(73)\n",
    "torch.manual_seed(73)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(73)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c3a67",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Data Collection: Data is collected and provided to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b172aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Include data EDA from Anitra's branch here\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Include data EDA from Anitra's branch here\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf7cbb7",
   "metadata": {},
   "source": [
    "### Data Pre-processing: Convert the musical scores into a format suitable for deep learning models. This involves converting the musical scores into MIDI files and applying data augmentation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd71a4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bach:\n",
      "Splitting music files: 100%|██████████| 1024/1024 [00:10<00:00, 79.55it/s]\n",
      "Performing data augmentation: 100%|██████████| 2263/2263 [00:24<00:00, 500.35it/s]\n",
      "Beethoven:\n",
      "Splitting music files: 100%|██████████| 220/220 [00:02<00:00, 98.92it/s]\n",
      "Performing data augmentation: 100%|██████████| 3004/3004 [00:27<00:00, 460.36it/s]\n",
      "Chopin:\n",
      "Splitting music files: 100%|██████████| 135/135 [00:01<00:00, 131.18it/s]\n",
      "Performing data augmentation: 100%|██████████| 11754/11754 [01:38<00:00, 119.72it/s]\n",
      "Mozart:\n",
      "Splitting music files: 100%|██████████| 257/257 [00:02<00:00, 104.82it/s]\n",
      "Performing data augmentation: 100%|██████████| 2588/2588 [00:04<00:00, 527.91it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Train a tokenizer on each composer, split the data by number of tokens and augment the data\"\"\"\n",
    "config = TokenizerConfig(use_chords=True, use_rests=True, use_tempos=True, use_programs=True)\n",
    "tokenizer = REMI(config)\n",
    "\n",
    "DATA_PATH = Path(Path.cwd().parent, \"Data\")\n",
    "RETAIN = 10000\n",
    "\n",
    "for composer in [\"Bach\", \"Beethoven\", \"Chopin\", \"Mozart\"]:\n",
    "\tprint(f\"{composer}:\")\n",
    "\tmidi_path = list(Path(DATA_PATH).glob(f\"{composer}/*.mid\"))\n",
    "\ttokenizer.train(vocab_size=30000, files_paths=midi_path)\n",
    "\tsubset_chunks_dir = Path(DATA_PATH, f\"{composer}_augmented\")\n",
    "\t\n",
    "\tsplit_files_for_training(\n",
    "        files_paths=midi_path,\n",
    "        tokenizer=tokenizer,\n",
    "        save_dir=subset_chunks_dir,\n",
    "        max_seq_len=1024,\n",
    "        num_overlap_bars=2,\n",
    "    )\n",
    "\n",
    "\taugment_dataset(\n",
    "\t\tsubset_chunks_dir,\n",
    "\t\tpitch_offsets=[-6, 6],\n",
    "\t\tvelocity_offsets=[-4, 4],\n",
    "\t\tduration_offsets=[-0.2, 0.2],\n",
    "\t\tsave_data_aug_report=False,\n",
    "\t\tall_offset_combinations=composer == \"Chopin\" # perform more augmentation if chopin (low token count)\n",
    "\t)\n",
    "\n",
    "\t# use RETAIN as upper bound of song count and remove excess at random to balance composer data\n",
    "\tall_files = list(Path(subset_chunks_dir).glob(\"*.mid\"))\n",
    "\tcomposer_sample = sample(all_files, len(all_files) - RETAIN)\n",
    "\tfor file_name in composer_sample:\n",
    "\t\tfile_name.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6a733",
   "metadata": {},
   "source": [
    "### Feature Extraction: Extract features from the MIDI files, such as notes, chords, and tempo, using music analysis tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Combine all MIDIs and train new tokenizer using combined data\"\"\"\n",
    "combined_midi_path = list(Path(DATA_PATH).glob(f\"*_augmented/*.mid\"))\n",
    "\n",
    "tokenizer = REMI(config)\n",
    "tokenizer.train(vocab_size=30000, files_paths=combined_midi_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613910d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting music files: 100%|██████████| 28000/28000 [00:45<00:00, 613.39it/s]\n",
      "Splitting music files: 100%|██████████| 6000/6000 [00:09<00:00, 606.37it/s]\n",
      "Splitting music files: 100%|██████████| 6000/6000 [00:09<00:00, 614.17it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Split into train/valid/test datasets using roughly 15% of the data for each of valid and test\"\"\"\n",
    "total_num_files = len(combined_midi_path)\n",
    "num_files_valid = round(total_num_files * 0.15)\n",
    "num_files_test = round(total_num_files * 0.15)\n",
    "shuffle(combined_midi_path)\n",
    "midi_paths_valid = combined_midi_path[:num_files_valid]\n",
    "midi_paths_test = combined_midi_path[num_files_valid:num_files_valid + num_files_test]\n",
    "midi_paths_train = combined_midi_path[num_files_valid + num_files_test:]\n",
    "\n",
    "for files_paths, subset_name in (\n",
    "    (midi_paths_train, \"Train\"), (midi_paths_valid, \"Validate\"), (midi_paths_test, \"Test\")\n",
    "):\n",
    "    subset_chunks_dir = Path(DATA_PATH, subset_name)\n",
    "    split_files_for_training(\n",
    "        files_paths=files_paths,\n",
    "        tokenizer=tokenizer,\n",
    "        save_dir=subset_chunks_dir,\n",
    "        max_seq_len=1024,\n",
    "        num_overlap_bars=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a57330",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train new tokenizer based on training data only and save the weights locally\"\"\"\n",
    "training_midi_path = list(Path(DATA_PATH).glob(f\"Train/**/*.mid\"))\n",
    "\n",
    "tokenizer = REMI(config)\n",
    "tokenizer.train(vocab_size=30000, files_paths=training_midi_path)\n",
    "\n",
    "tokenizer.save_pretrained(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeee5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Load the tokenizer using the locally saved weights\"\"\"\n",
    "DATA_PATH = Path(Path.cwd().parent, \"Data\")\n",
    "tokenizer = REMI(TokenizerConfig(use_chords=True, use_rests=True, use_tempos=True, use_programs=True)).from_pretrained(\"tokenizer.json\")\n",
    "tokenizer.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c7613",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create torch compatable data loaders using the datasets created above\"\"\"\n",
    "class Capture:\n",
    "\tdef __init__(self):\n",
    "\t\tself.score = None\n",
    "\t\tself.tok_sequence = None\n",
    "\t\n",
    "\tdef lable_composer(self, score, tok_sequence, file_path):\n",
    "\t\tself.score = score\n",
    "\t\tself.tok_sequence = tok_sequence\n",
    "\t\tcomposer = file_path.parts[-2:-1]\n",
    "\t\tif \"Bach\" in composer:\n",
    "\t\t\treturn 0\n",
    "\t\telif \"Beethoven\" in composer:\n",
    "\t\t\treturn 1\n",
    "\t\telif \"Chopin\" in composer:\n",
    "\t\t\treturn 2\n",
    "\t\telif \"Mozart\" in composer:\n",
    "\t\t\treturn 3\n",
    "\t\telse:\n",
    "\t\t\treturn -1\n",
    "\t\t\n",
    "capture = Capture()\n",
    "collator = DataCollator(tokenizer.pad_token_id)\n",
    "\n",
    "dataset_train = DatasetMIDI(\n",
    "    files_paths=list(Path(DATA_PATH, \"Train\").glob(\"**/*.mid\")),\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    "    func_to_get_labels=capture.lable_composer\n",
    ")\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=64, collate_fn=collator, shuffle=True)\n",
    "\n",
    "dataset_valid = DatasetMIDI(\n",
    "    files_paths=list(Path(DATA_PATH, \"Valid\").glob(\"**/*.mid\")),\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=64, collate_fn=collator, shuffle=True)\n",
    "\n",
    "dataset_test = DatasetMIDI(\n",
    "    files_paths=list(Path(DATA_PATH, \"Test\").glob(\"**/*.mid\")),\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=64, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744bc639",
   "metadata": {},
   "source": [
    "### Model Building: Develop a deep learning model using LSTM and CNN architectures to classify the musical scores according to the composer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e4c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LSTM and CNN definitions\"\"\"\n",
    "class ComposerLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3):\n",
    "        super(ComposerLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        c_0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h_0, c_0))  # out: [batch, seq_len, hidden]\n",
    "        out = out[:, -1, :]  # Take last timestep\n",
    "        out = nn.functional.relu(self.fc1(out))\n",
    "        return self.fc2(out)\n",
    "    \n",
    "class composerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(composerCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.fc2 = nn.Linear(256, 32)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return nn.functional.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952b0d66",
   "metadata": {},
   "source": [
    "### Model Training: Train the deep learning model using the pre-processed and feature-extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f7290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define training loop\"\"\"\n",
    "def train(model, dataset, num_epochs=300, lr=0.001):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e520b",
   "metadata": {},
   "source": [
    "### Model Evaluation: Evaluate the performance of the deep learning model using accuracy, precision, and recall metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366a71a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66bc4fd0",
   "metadata": {},
   "source": [
    "### Model Optimization: Optimize the deep learning model by fine-tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34b7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1da6260f",
   "metadata": {},
   "source": [
    "## Findings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
